{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16d0f83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc8abbe",
   "metadata": {},
   "source": [
    "## Policy Gradient Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d8b2fc",
   "metadata": {},
   "source": [
    "We calculate the gradient with respect to the control parameter when doing the optimization. \n",
    "\n",
    "$$\\pi(a|s)$$\n",
    "\n",
    "Policy and trajectory $\\tau$, and this is one path. \n",
    "\n",
    "$$s_1 \\rightarrow a_1 \\rightarrow  s_2 \\rightarrow a_2 \\rightarrow ... \\rightarrow s_{T-1} \\rightarrow a_{T-1} \\rightarrow s_T \\rightarrow a_T$$\n",
    "\n",
    "such chain depends on both the transition probability and the policy\n",
    "1. $p(s_{t+1}|s_t,a)$\n",
    "2. $\\pi_{\\theta}(a_t|s_t)$\n",
    "As a result, heurestically, the probability law of chains can be written in the following fashion\n",
    "$$\n",
    "\\begin{equation}\n",
    "p_{\\theta}(\\tau)= p(s_1) \\prod^T_{t=1} \\pi(a_{t}|s_{t}) p( s_{t+1} |a_{t}, s_{t} )\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "In this case, the objective function can be put in the following form: \n",
    "$$\n",
    "\\begin{equation}\n",
    "J(\\theta) = \\mathbb{E}_{\\tau \\sim p(\\theta)} [\\sum r(s_t, a_t)]\n",
    "\\end{equation}\n",
    "$$\n",
    "and the goal now is to find the $\\theta^*$ that maximize this reward. \n",
    "\n",
    "Now, we are going to take derivatives with respect to the parameter $\\theta$:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\nabla_{\\theta} J(\\theta) &= \\int \\nabla_{\\theta} p_{\\theta}(\\tau) r(\\tau) d\\tau \\\\\n",
    "&= \\int p_{\\theta}(\\tau) (\\nabla_{\\theta} \\log p_{\\theta}(\\tau) r(\\tau)) d \\tau \\\\ \n",
    "&= \\int  p_{\\theta}(\\tau) r(\\tau) \\nabla_{\\theta}\\lbrace log p(s_1)+ \\sum^T_{t=1}[\\log \\pi_{\\theta}(a_t|s_t) +\\log p(s_{t+1}|s_t,a_t)] \\rbrace \\\\\n",
    "&= \\mathbb{E}_{\\tau \\sim p_{\\theta}(\\tau)}[(\\sum^T_{t=1} \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t | s_t) ) (\\sum^T_{t=1} r(s_t,a_t))]\\\\\n",
    "& \\approx \\frac{1}{N} \\sum^N_{i=1} \\big \\lbrace \\sum^T_{t=1} \\nabla_{\\theta} \\log \\pi_{\\theta}(a^i_t | s^i_t) ) (\\sum^T_{t=1} r(s^i_t,a^i_t)) \\big \\rbrace\n",
    "\\end{align}\n",
    "$$\n",
    "And of course, the next step is to do a gradient descent \n",
    "$$\n",
    "\\begin{align}\n",
    "\\theta = \\theta + \\alpha \\nabla_{\\theta} J(\\theta)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Thus, the algorithm is natural: sample $(\\tau')$ a set of N trajectories from current policy $\\pi_{\\theta}(a_t|s)t)$ and update the parameters, and do the iterations. \n",
    "\n",
    "We comment here that the forumalation here follows a variational approach and does not depend on the Bellman's formulation\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c12b0bd",
   "metadata": {},
   "source": [
    "### Variance Reduction\n",
    "\n",
    "Noticing that the future states will not impact the historical ones (the world is casual), we make the following changes\n",
    "$$\n",
    "\\begin{align}\n",
    "\\nabla_{\\theta} J(\\theta) \n",
    "&= \\mathbb{E}_{\\tau \\sim p_{\\theta}(\\tau)}[\\sum^T_{t=1} (\\nabla_{\\theta} \\log \\pi_{\\theta}(a_t | s_t)  \\sum^T_{{t'}=t} r(s_{t'},a_{t'}))]\\\\\n",
    "& \\approx \\frac{1}{N} \\sum^N_{i=1}  \\sum^T_{t=1} \\big \\lbrace \\nabla_{\\theta} \\log \\pi_{\\theta}(a^i_t | s^i_t)  \\sum^T_{{t'}=t} r(s^i_{t'},a^i_{t'})) \\big \\rbrace\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Then, we can write the following \n",
    "$$Q^i_t:=\\sum^T_{{t'}=t} r(s^i_{t'},a^i_{t'})$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04049b0",
   "metadata": {},
   "source": [
    "## Regularization \n",
    "\n",
    "This is to ensure that the policy that we learn does not collapse to a single strategy.\n",
    "\n",
    "The regularity term is defined as \n",
    "$$H(x)= \\sum_x - p(x) \\log p(x)$$\n",
    "\n",
    "In this case, $p(x):= \\pi_{\\theta}(a^i_t|s^i_t)$ and higher entropy would mean the distribution is more spread out. \n",
    "\n",
    "$$\\begin{align}\n",
    "L(\\theta):=-\\frac{1}{N} \\sum^N_{i=1} \\bigg[  \\sum^T_{t=1} \\big \\lbrace \\nabla_{\\theta} \\log \\pi_{\\theta}(a^i_t | s^i_t)  \\sum^T_{{t'}=t} \\gamma^{t'-t}r(s^i_{t'},a^i_{t'})) \\big \\rbrace -\\beta \\sum_{a_i} \\pi_{\\theta}(a^i_t|s^i_t)\\log\\pi_{\\theta}(a^i_t|s^i_t) \\bigg]\n",
    "\\end{align}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a5c421",
   "metadata": {},
   "source": [
    "To perform variance reduction to make sure that the sampling is more effective, the following adjustment is proposed. \n",
    "$$\n",
    "\\begin{align}\n",
    "\\nabla_{\\theta} J(\\theta) = \\mathbb{E}_{\\tau \\sim p_{\\theta}(\\tau)}[\\big( \\sum^T_{t=1} \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t | s_t) \\big) (r(\\tau)-b(s_t))]\n",
    "\\end{align}\n",
    "$$\n",
    "Notice that $b(\\cdot)$ should only be a function of the state variable. \n",
    "\n",
    "We comment that reinforcement and all the variations are on-policy algorithms and so the trajectories created before the policy updates are dated. \n",
    "\n",
    "Consider the advantage function \n",
    "\n",
    "$$\\hat{Q}(s^i_t,a^i_t) -b^i(s_t)$$\n",
    "with before the $Q^i_t:=\\sum^T_{{t'}=t} r(s^i_{t'},a^i_{t'})$, now we can do a rollout, and take $$Q(s^i_t,a^i_t) =r(s^i_t,a^i_t)+V(s_{t+1})$$. Then, we have two approaches \n",
    "1. MC approach $$\\nabla_{\\theta} J(\\theta)=\\frac{1}{N} \\sum^N_{i=1}  \\sum^T_{t=1} \\bigg \\lbrace \\nabla_{\\theta} \\log \\pi_{\\theta}(a^i_t | s^i_t)  \\big[\\sum^T_{{t'}=t}  r(s^i_{t'},a^i_{t'})) - V(s_t) \\big] \\bigg \\rbrace$$\n",
    "2. TD approach $$\\nabla_{\\theta} J(\\theta)=\\frac{1}{N} \\sum^N_{i=1}  \\sum^T_{t=1} \\bigg \\lbrace \\nabla_{\\theta} \\log \\pi_{\\theta}(a^i_t | s^i_t)  \\big[ r(s^i_{t},a^i_{t})) + V(s_{t+1})- V(s_{t})\\big] \\bigg \\rbrace$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c739172e",
   "metadata": {},
   "source": [
    "## Algorithm\n",
    "Approximate $\\pi_{\\theta}(a|s)$ and $V_{\\phi}(s)$ using two different neural networks. \n",
    "\n",
    "Loop: \n",
    "1. Sample N trajectories from the current policy $\\pi_{\\theta}(a_t|s_t)$. \n",
    "2. Calculate the $\\hat{Q}^i_t=\\sum^T_{{t'}=t}  r(s^i_{t'},a^i_{t'})$. and fit it with the approximated neural network $V_{\\phi}(s)$ (L2 loss). \n",
    "\n",
    "$$\\phi= \\phi - \\beta \\nabla_{\\phi} L$$\n",
    "3. compute the cross-entropy loss: \n",
    "$$J(\\theta):=\\frac{1}{N} \\sum^N_{i=1}  \\sum^T_{t=1} \\bigg \\lbrace \\nabla_{\\theta} \\log \\pi_{\\theta}(a^i_t | s^i_t)  \\big[ r(s^i_{t},a^i_{t})) + V_{\\phi}(s_{t+1})- V_{\\phi}(s_{t})\\big] \\bigg \\rbrace$$\n",
    "Perform gradient descent on $\\theta$: \n",
    "$$\\theta=\\theta+ \\alpha \\nabla_{\\theta} J(\\theta) $$\n",
    "\n",
    "We comment that entropic regularization can also be applied to this current framework."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fe74fc",
   "metadata": {},
   "source": [
    "# A combination of Policy gradient descent and Q learning \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0099ed1e",
   "metadata": {},
   "source": [
    "1. Q-learning can be unstable sometimes. Though it is off policy and so the transitioning samples can be used multiple times. \n",
    "2. Learning policy directly gives much better convergence guarantees. However they are on-policy.\n",
    "\n",
    "Three methods are in scope. Deep deterministic policy gradients (DDPG), twin delayed DDPG (TD3) and soft actor critic (SAC). \n",
    "\n",
    "#### Weakness of the DQN \n",
    "1. It learns the action-value value function instead of the policy. \n",
    "2. It learns a moving target. \n",
    "3. Convergence is not guaranteed. \n",
    "4. Can only tackle discrete problems.\n",
    "\n",
    "\n",
    "\n",
    "#### Weakness of Policy Gradient\n",
    "1. Collapse of learned results to a bad region. (TRPO and PPO proximal... to improve.)\n",
    "2. It is on policy, so it is sample inefficient.  \n",
    "3. Policy is the actor while the value network is the critic, but the learning is still on policy. One uses the critic to guide the actor, but one still has to discard all the transitions after an update to the policy network\n",
    "\n",
    "## General framework. \n",
    "Assume that the policy network is parameterized by $\\theta$, and the action is $a=\\mu_{\\theta}(s)$ (notice that in the control context, this is standard feedback control)\n",
    "$$\\max_{a'}Q^*(s',a') \\approx Q^*(s', \\mu_{\\theta}(s))$$\n",
    "The critic network will take the form $Q_{\\theta}(s,\\mu_{\\theta}(s))$\n",
    "one wants to take action $a=\\mu_{\\theta}(s)$, but then we add a little noise for regularization, set $\\epsilon \\sim N(0, \\sigma^2)$ and use $a+ \\epsilon$ action to explore the environment and generate samples. Now one can generate it in an off-line fashion and the samples can be stored in a buffer. \n",
    "\n",
    "The approach of updating the target neural network is the polyak averaging (exponential averaging). \n",
    "$$\n",
    "\\begin{equation}\n",
    "\\phi_{target} \\leftarrow \\rho \\phi_{target}+(1-\\rho) \\phi\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "The loss function is now \n",
    "$$\n",
    "\\begin{equation}\n",
    "L(\\phi, D)= \\mathbb{E}_{(s,a,s',d)\\sim D} \\bigg[ \\big( Q_{\\phi}(s,a)-(r+\\gamma(1-d) \\max_{a'}Q_{\\phi_{trg}(s',a')})  \\big)^2 \\bigg]\n",
    "\\end{equation}\n",
    "$$\n",
    "We replace the optimality condition with a desense enural network \n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "L(\\phi, D)= \\mathbb{E}_{(s,a,s',d)\\sim D} \\bigg[ \\bigg( Q_{\\phi}(s,a)-\\big (r+\\gamma(1-d) Q_{\\phi_{trg}}(s',\\mu_{\\theta}(s')) \\big)  \\bigg)^2 \\bigg]\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "This is for the value function optimization. We also need a policy update criteria. since we have assumed that the $\\mu_{\\theta}(s)$ is the optimal one, we have \n",
    "$$\\max_{\\theta}J(\\theta, D) = \\max_{\\theta} \\mathbb{E}_{s\\sim D} \\big[ Q_{\\phi} \\Big( s, \\mu_{\\theta}(s) \\Big ) \\big]$$\n",
    "And the gradient is simple in this case \n",
    "$$\n",
    "\\nabla_{\\theta}J(\\theta, D) = \\mathbb{E}_{s\\sim D} \\big[ \\nabla_a Q_{\\phi} ( s, a ) |_{\\mu_{\\theta}(s)} \\nabla_{\\phi} \\mu_{\\theta} (s) \\big]\n",
    "$$\n",
    "\n",
    "***\n",
    "**Deep Deterministic Policy Gradient**\n",
    "***\n",
    " \n",
    "1. Input initial policy parameters $\\theta$,  Q-function parameters $\\phi$, empty replay buffer D\n",
    "\n",
    "2. Set target parameters equal to online parameters $\\theta_{targ} \\leftarrow \\theta$ and $\\phi_{targ} \\leftarrow \\phi$\n",
    "\n",
    "3. **repeat**\n",
    "\n",
    "4. Observe state s and select action $a = \\mu_\\theta(s)+\\epsilon, \\text{where  } \\epsilon \\sim N$, restrict it to the set $(a_{Low}, a_{High})$\n",
    "\n",
    "5. Execute a in environment and observe next state s', reward r, and done signal d\n",
    "\n",
    "6. Store `(s,a,r,s',d)` in Replay Buffer D\n",
    "\n",
    "7. if `s'` is terminal state, reset the environment\n",
    "\n",
    "8. if it's time to update **then**:\n",
    "\n",
    "9. &emsp;&emsp;for as many updates as required:\n",
    "\n",
    "10. &emsp;&emsp;&emsp;&emsp;Sample a batch B={`(s,a,r,s',d)`} from replay Buffer D:\n",
    "\n",
    "11. &emsp;&emsp;&emsp;&emsp;Compute targets: $$y(r,s',d) = r + \\gamma(1-d)Q_{targ}(s',\\mu_{\\theta_{targ}}(s'))$$\n",
    "\n",
    "12. &emsp;&emsp;&emsp;&emsp;Update Q function with one step gradient descent on $\\phi$: $$\\nabla_\\phi \\frac{1}{|B|} \\sum_{(s,a,r,s',d)\\in B}(Q_\\phi(s,a) - y(r,s',d))^2$$\n",
    "\n",
    "13. &emsp;&emsp;&emsp;&emsp;Update Policy with one step gradient Ascent on $\\theta$: $$\\nabla_\\theta \\frac{1}{|B|} \\sum_{s \\in B} Q_\\phi(s, \\mu_\\phi(s, \\mu_\\theta(s))$$\n",
    "\n",
    "14. Update target networks using polyak averaging: $$\\phi_{targ} \\leftarrow \\rho\\phi_{targ} + (1-\\rho)\\phi$$  $$\\theta_{targ} \\leftarrow \\rho\\theta_{targ} + (1-\\rho)\\theta$$\n",
    "***\n",
    "\n",
    "The algorithm is interesting. One sets up the target with $\\theta_{target}$, $\\phi_{target}$ being updated dynamically. They are updated by using the polyak averaging. (Why is it the target??? Is there any convergence guarantee?) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b0a3ef",
   "metadata": {},
   "source": [
    "### Twin Delayed DDPG (TD3)\n",
    "DDPG suffers from the overestimation bias that one observes from the Q-learning.\n",
    "https://arxiv.org/pdf/1802.09477.pdf\n",
    "\n",
    "The above paper proposes a variant of double Q-learning. The following modifications are taken:\n",
    "1. Clipped double Q-learning: TD3 uses two independent Q-functions and takes the minimum of. the two while forming targets under Bellman equations. \n",
    "2. Delayed policy updates: TD3 updates the polciy and target networks less frequently as compared to the Q-function updates. \n",
    "3. Target policy smoothing: TD3 adds noise to the target action, making it harder for the policy to exploit Q-function estimation error and contorl the overestimation bias. \n",
    "\n",
    "***\n",
    "**Twin Delayed DDPG (TD3)**\n",
    "***\n",
    " \n",
    "1. Input initial policy parameters $\\theta$,  Q-function parameters $\\phi_1$ and $\\phi_2$, empty replay buffer D\n",
    "\n",
    "2. Set target parameters equal to online parameters $\\theta_{targ} \\leftarrow \\theta$, $\\phi_{targ,1} \\leftarrow \\phi_1$ and $\\phi_{targ,2} \\leftarrow \\phi_2$\n",
    "\n",
    "3. **repeat**\n",
    "\n",
    "4. Observe state s and select action $a = clip(\\mu_\\theta(s)+\\epsilon, a_{Low}, a_{High}), \\text{where  } \\epsilon \\sim N$\n",
    "\n",
    "5. Execute a in environment and observe next state s', reward r, and done signal d\n",
    "\n",
    "6. Store `(s,a,r,s',d)` in Replay Buffer D\n",
    "\n",
    "7. if `s'` is terminal state, reset the environment\n",
    "\n",
    "8. if it's time to update **then**:\n",
    "\n",
    "9. &emsp;&emsp;for j in range (as many updates as required):\n",
    "\n",
    "10. &emsp;&emsp;&emsp;&emsp;Sample a batch B={`(s,a,r,s',d)`} from replay Buffer D:\n",
    "\n",
    "11. &emsp;&emsp;&emsp;&emsp;Compute target actions:\n",
    "\n",
    "$$a'(s') = \\text{clip}\\left(\\mu_{\\theta_{\\text{targ}}}(s') + \\text{clip}(\\epsilon,-c,c), a_{Low}, a_{High}\\right), \\;\\;\\;\\;\\; \\epsilon \\sim \\mathcal{N}(0, \\sigma)$$\n",
    "\n",
    "12. &emsp;&emsp;&emsp;&emsp;Compute action targets: \n",
    "\n",
    "$$y(r,s',d) = r + \\gamma (1-d) \\min_{i=1,2} Q_{\\phi_{\\text{targ},i}}(s', a'(s'))$$\n",
    "\n",
    "13. &emsp;&emsp;&emsp;&emsp;Update Q function with one step gradient descent on $\\phi$: \n",
    "$$\\nabla_\\phi \\frac{1}{|B|} \\sum_{(s,a,r,s',d)\\in B}(Q_{\\phi_i}(s,a) - y(r,s',d))^2, \\;\\;\\;\\;\\;  \\text{for } i=1,2$$\n",
    "\n",
    "14. &emsp;&emsp;&emsp;&emsp;if `j mod policy_update == 0`:\n",
    "\n",
    "15. &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;Update Policy with one step gradient Ascent on $\\theta$: \n",
    "\n",
    "$$\\nabla_\\theta \\frac{1}{|B|} \\sum_{s \\in B} Q_{\\phi_1}(s, \\mu_\\phi(s, \\mu_\\theta(s))$$\n",
    "\n",
    "16. &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;Update target networks using polyak averaging: \n",
    "$$\\phi_{targ,i} \\leftarrow \\rho\\phi_{targ,i} + (1-\\rho)\\phi_i, \\;\\;\\;\\;\\;  \\text{for } i=1,2$$ \n",
    "$$\\theta_{targ} \\leftarrow \\rho\\theta_{targ} + (1-\\rho)\\theta$$\n",
    "***\n",
    "Similar to DDPG:\n",
    "1. TD3 is an off-policy algorithm.\n",
    "2. TD3 can only be used for environments with continuous action spaces.\n",
    "3. TD3 can be thought of as being deep Q-learning for continuous action spaces.\n",
    "\n",
    "\n",
    "This notebook follows the code found in OpenAI's [Spinning Up Library](https://spinningup.openai.com/en/latest/algorithms/td3.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a071dc7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52bf603",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09597f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cfc850",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82271d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e101aba9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
