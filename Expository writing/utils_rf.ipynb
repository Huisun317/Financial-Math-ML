{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9606ab18",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random\n",
    "import gym \n",
    "import os\n",
    "import time\n",
    "import glob \n",
    "import import_ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6130e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "MOMENTUM = 0.99\n",
    "EPSILON = 1e-6\n",
    "device= torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca35d92c",
   "metadata": {},
   "source": [
    "## Replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5272b900",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The startegy is to append the (S,A,R,S) item to the buffer. \n",
    "class ReplayBuffer: \n",
    "    def __init__(self,dim):\n",
    "        self.buffer=[]\n",
    "        self.dim=dim\n",
    "    def __len__(self): # extend the existing function len to the current\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    def add(self,state,action,reward,next_state,done):\n",
    "        sarsd=(state,action,reward,next_state,done)\n",
    "        if len(self.buffer) < self.dim:\n",
    "            self.buffer.append(sarsd)\n",
    "        else:\n",
    "            self.buffer.append(sarsd)\n",
    "            self.buffer.pop(0)\n",
    "            \n",
    "    def sample(self,batch_size): \n",
    "        batch_index=np.random.choice(len(self.buffer),batch_size)\n",
    "        batch_samples=[self.buffer[idx] for idx in batch_index]\n",
    "        b_state,b_action,b_reward,b_nstate,b_ndone= list(zip(*batch_samples)) # Rearange the list by type\n",
    "        return np.array(b_state), np.array(b_action),np.array(b_reward),np.array(b_nstate),np.array(b_ndone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7bc1db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_and_record(start_state, agent, env, exp_replay, n_steps=1):\n",
    "    s=start_state\n",
    "    sum_rewards=0\n",
    "    \n",
    "    for _ in range(n_steps):\n",
    "        qvalues=agent.get_qvalues([s])\n",
    "        a = agent.sample_actions(qvalues)[0]\n",
    "        \n",
    "        #the standard output contains the next_state, reward, done\n",
    "        next_s, r, done, _ =env.step(a)\n",
    "        \n",
    "        sum_rewards+=r\n",
    "        exp_replay.add(s,a,r,next_s,done)\n",
    "        if done:\n",
    "            s=env.reset()\n",
    "        else:\n",
    "            s=next_s\n",
    "        \n",
    "    return sum_rewards, s  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149d9c69",
   "metadata": {},
   "source": [
    "## Dense Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec84bf09",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function defines one block for the nn \n",
    "\"\"\"\n",
    "class Dense(nn.Module): \n",
    "    def __init__(self,cin, cout, batch_norm=False, activate=True): \n",
    "        super(Dense,self).__init__()\n",
    "        self.cin=cin; \n",
    "        self.cout=cout; \n",
    "        self.activate=activate; \n",
    "        \n",
    "        self.linear=nn.Linear(self.cin,self.cout) #The linear layer\n",
    "        #BatchNorm1d: it requires the input to be a correct size\n",
    "        if batch_norm: \n",
    "            self.bn=nn.BatchNorm1d(cout,eps=EPSILON,momentum=MOMENTUM)\n",
    "        else: \n",
    "            self.bn=None\n",
    "      #  nn.init.normal_(self.linear.weight,std=5.0/np.sqrt(cin+cout))\n",
    "        # This is the He initialization\n",
    "        \n",
    "    def forward(self,x): \n",
    "        x=self.linear(x)\n",
    "        if self.bn is not None and x.shape[0]!=1:\n",
    "            x=self.bn(x)\n",
    "        if self.activate:\n",
    "            x=torch.relu(x)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "591087e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseNet(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(DenseNet,self).__init__()\n",
    "        self.config=config\n",
    "        \n",
    "        self.bn=nn.BatchNorm1d(config.num_hiddens[0],eps=EPSILON,momentum=MOMENTUM) ## So there is batch norm no problem\n",
    "        # range(1,5): 1,2,3,4\n",
    "        self.layers=[Dense(config.num_hiddens[i-1],config.num_hiddens[i]) for i in range(1, len(config.num_hiddens)-1)]\n",
    "        self.layers+=[Dense(config.num_hiddens[-2], config.num_hiddens[-1],activate=False)]\n",
    "        self.layers=nn.Sequential(*self.layers)\n",
    "    \n",
    "    def forward(self,x):\n",
    "     #   if x.shape[0]!=1:\n",
    "      #      x=self.bn(x) \n",
    "        ## Interestingly the batchnorm is not supposed to be used in this case\n",
    "        x=self.layers(x)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6831b71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c1f0a75c",
   "metadata": {},
   "source": [
    "## Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b1e4bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The sarsa input\n",
    "\n",
    "def compute_td_loss(agent, target_network, states, actions, rewards, next_states, done_flags,\n",
    "                    gamma=0.99, device=device):\n",
    "\n",
    "    # convert numpy array to torch tensors\n",
    "    states = torch.tensor(states, device=device, dtype=torch.float)\n",
    "    actions = torch.tensor(actions, device=device, dtype=torch.long)\n",
    "    rewards = torch.tensor(rewards, device=device, dtype=torch.float)\n",
    "    next_states = torch.tensor(next_states, device=device, dtype=torch.float)\n",
    "    done_flags = torch.tensor(done_flags.astype('float32'),device=device,dtype=torch.float)\n",
    "\n",
    "    # get q-values for all actions in current states\n",
    "    # use agent network\n",
    "    # Dimension is 32x2\n",
    "    predicted_qvalues = agent(states)\n",
    "\n",
    "    # compute q-values for all actions in next states\n",
    "    # use target network\n",
    "    # Dimension is 32x2\n",
    "    predicted_next_qvalues = target_network(next_states)\n",
    "    \n",
    "    # select q-values for chosen actions\n",
    "    # dimension is now 32x1\n",
    "    predicted_qvalues_for_actions = predicted_qvalues[range(\n",
    "        len(actions)), actions]\n",
    "\n",
    "    # compute Qmax(next_states, actions) using predicted next q-values\n",
    "    next_state_values,_ = torch.max(predicted_next_qvalues, dim=1)\n",
    "\n",
    "    # compute \"target q-values\" \n",
    "    target_qvalues_for_actions = rewards + gamma * next_state_values * (1-done_flags)\n",
    "\n",
    "    loss = torch.mean((predicted_qvalues_for_actions -\n",
    "                       target_qvalues_for_actions.detach()) ** 2)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5d01e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9cb8cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663ed680",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e90194b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfdc071",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3dad77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b4794e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfa4233",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7407c6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e362c3f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
