{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3736f683",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import numpy as np \n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1f5f8b",
   "metadata": {},
   "source": [
    "# Model-Free Approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b9684c",
   "metadata": {},
   "source": [
    "In most of the real life problems, transition dynamics are not available. Hence, sampling works as the major technique. \n",
    "The methods are in consideration, namely the MC method and TD method \n",
    "1. Bootstrapping \n",
    "2. Exploration-vs-exploitation\n",
    "3. Off-Policy vs on Policy\n",
    "For the MC method, expected returns are calculated by using the average of sample returns. One typically needs to know the whole trajectory of the starting point all the way to the terminate point. \n",
    "\n",
    "`It does not work for the non-terminating case`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75671c6",
   "metadata": {},
   "source": [
    "## First Visit MC\n",
    "\n",
    "First visit, then terminate the state. \n",
    "\n",
    "$v(s)$-current estimate, $N(s)$-number of visit. \n",
    "\n",
    "Initiate: \n",
    "\n",
    "1. Cumulative state val: $S(s)=0, \\forall s \\in \\mathbf{S}$\n",
    "2. Estimated state val: $v(s)=0,  \\forall s \\in \\mathbf{S}$\n",
    "3. Visit count: $N(s)=0,  \\forall s \\in \\mathbf{S}$\n",
    "\n",
    "Loop:\n",
    "\n",
    "Sample from Policy $\\pi$: $S_0, A_0, R_1, S_1, A_1, R_2, ..., R_T,S_T$ `Note: one can see that some state may never be visited`\n",
    "\n",
    "$G \\leftarrow 0$\n",
    "\n",
    "Loop backward for each step of episode: $t=T-1, T-2, ..., 1, 0$\n",
    "\n",
    "$$G \\leftarrow G + R_{t+1}$$\n",
    "\n",
    "If $S_t$ does not appear in $S_0, S_1, ..., S_{t-1}$:  ($\\textbf{First Visit Cond}$)\n",
    "$$\n",
    "\\begin{align}\n",
    "N(s) &\\leftarrow N(s)+1 \\\\ \n",
    "S(s) &\\leftarrow S(s)+G \\\\ \n",
    "v(s) &\\leftarrow S(s)/N(s)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We comment that the same strategy can also be used on the state action function $q(s,a)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe7c76a",
   "metadata": {},
   "source": [
    "## Every visit MC\n",
    "\n",
    "`For the every visit algorithm, just remove the` ($\\textbf{First Visit Cond}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa986c7",
   "metadata": {},
   "source": [
    "## Observations and comments\n",
    "\n",
    "By definition, we have the following:\n",
    "$$\n",
    "\\begin{align}\n",
    "N_{n+1}&=N(s)\\\\\n",
    "v_{n+1}(s)&=[S_n(s)+G]/N_{n+1}(s)\\\\\n",
    "&=[v_n(s)N_n(s) +G]/N_{n+1}(s) \\\\ \n",
    "&=v_n(s)+\\frac{1}{N_{n+1}(s)}[G-v_n(s)]\n",
    "\\end{align}\n",
    "$$\n",
    "And it suggests a very general updating form\n",
    "$$\n",
    "\\begin{equation}\n",
    "v_{n+1} = v_{n} + \\alpha (G-v_n)  \\tag{1} \\label{eq1}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Like we pointed out before, some of the (state,action) pair may never be visited due to the nature of Monte Carlo sampling strategy. However, when we try to find the improvement, we do \n",
    "$$argmax_{a} q(s,a)$$\n",
    "then, in this case some of the pairs may never be known.\n",
    "Since the standard strategy for optimization under the current framework is \n",
    "$$\n",
    "\\begin{cases}\n",
    "    1. & \\text{Evaluate the policy} \\\\\n",
    "    2. & \\text{Policy improvement}\n",
    "\\end{cases}\n",
    "$$\n",
    "one is not supposed to do greedy improvement. In this case, one needs to balance between exploration and exploitation, and this is achieved by using the $\\epsilon$-greedy policy. That is, instead of taking the 'optimal action', one still explores other actions with some probability\n",
    "$$\n",
    "\\pi(a|s)=\n",
    "\\begin{cases}\n",
    "    1-\\epsilon + \\frac{\\epsilon}{|A|}. & \\text{for  } a = argmax_a Q(s,a) \\\\\n",
    "     \\frac{\\epsilon}{|A|} & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "We could run MC prediction followed by policy improvement on an episode-by-episode basis. This approach will remove the need for a large number of iterations in the estimation/prediction step, thus making the scheme scalable for Markove Decision Processes. \n",
    "\n",
    "But of course, for convergence, one would reduce the exploration size by some factor indexed by $k$. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e885bb3",
   "metadata": {},
   "source": [
    "## Greedy in the limit with inifite exploration\n",
    "\n",
    "First visit, then terminate the state. \n",
    "\n",
    "$q(s,a)$-state action function, $N(s,a)$-number of visit. \n",
    "\n",
    "Initiate: \n",
    "\n",
    "\n",
    "1. Estimated state val: $q(s,a)=0,  \\forall s \\in \\mathbf{S}$\n",
    "2. Visit count: $N(s,a)=0,  \\forall s \\in \\mathbf{S}$\n",
    "3. Policy $\\pi$ with enough exploration.\n",
    "\n",
    "Loop:\n",
    "\n",
    "Sample from Policy $\\pi_k$: $S_0, A_0, R_1, S_1, A_1, R_2, ..., R_T,S_T$ \n",
    "\n",
    "$G \\leftarrow 0$\n",
    "\n",
    "Loop backward for each step of episode: $t=T-1, T-2, ..., 1, 0$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "G &\\leftarrow \\gamma G + R_{t+1} \\\\\n",
    "N(s,a) &\\leftarrow N(s,a)+1 \\\\ \n",
    "q(s,a) &\\leftarrow q(s,a)+[G-q(s,a)]/N(s,a)\n",
    "\\end{align}\n",
    "$$\n",
    "Update the policy for convergence:$\\epsilon=\\frac{1}{k}$\n",
    " and using updated $q(s,a)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae58db1",
   "metadata": {},
   "source": [
    "### Off-policy MC control\n",
    "\n",
    "The previous algorithm(s) uses the same policy to explore and the one to be optimized. Such method is called the on-policy. \n",
    "\n",
    "There is another approach where the samples are generated by using a policy that is more exploratory with a higher $\\epsilon$ while the one being optimzied is the one with lower $\\epsilon$ or even a fully deterministic one. \n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "    1. & \\text{Used to generate samples ---behavior policy} \\\\\n",
    "    2. & \\text{Being optimized ---Target Policy}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "On policy can only find the optimal policy from the samples while off-policy algorithm can learn policy from data generated using other sub-optimal policies.\n",
    "### `Algorithm off-policy`\n",
    "\n",
    "1. Estimated state val: $q(s,a)=0,  \\forall s \\in \\mathbf{S}$\n",
    "2. Visit count: $N(s,a)=0,  \\forall s \\in \\mathbf{S}$\n",
    "3. Policy $\\pi=argmax_a Q(s,a)$.\n",
    "\n",
    "Loop:\n",
    "\n",
    "$b\\leftarrow a$ 'behavior policy' with enough exploration\n",
    "\n",
    "Sample episode (k) from Policy $\\pi_k$: $S_0, A_0, R_1, S_1, A_1, R_2, ..., R_T,S_T$ \n",
    "\n",
    "$G \\leftarrow 0$\n",
    "\n",
    "Loop backward for each step of episode: $t=T-1, T-2, ..., 1, 0$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "G &\\leftarrow \\gamma G + R_{t+1} \\\\\n",
    "N(s,a) &\\leftarrow N(s,a)+1 \\\\ \n",
    "q(s,a) &\\leftarrow q(s,a)+[G-q(s,a)]/N(s,a)\n",
    "\\end{align}\n",
    "$$\n",
    "$\\pi = argmax_a q(s,a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca5fbb6",
   "metadata": {},
   "source": [
    "# Temporal Differencing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135d1be1",
   "metadata": {},
   "source": [
    "The value of $v_{\\pi}(s)$ is estimated based on the current estimated states $v_{\\pi}(s')$, and such method is known as bootstrapping. \n",
    "\n",
    "Temporal differencing combines both the DP and the MC method together using bootstrapping\n",
    "$$\n",
    "\\begin{align}\n",
    "v(s) = v(s) + \\alpha [ \\underbrace{R + \\gamma v(s')}_{\\text{the original } G } -v(s)] \\tag{2} \\label{eq2}\n",
    "\\end{align}\n",
    "$$\n",
    "In \\eqref{eq2}, $s'$ is the next state. Such approach is called TD($0$).\n",
    "\n",
    "### `Algorithm TD(0)`\n",
    "1. Estimated state val: $q(s,a)=0,  \\forall s \\in \\mathbf{S}$\n",
    "2. Visit count: $N(s,a)=0,  \\forall s \\in \\mathbf{S}$\n",
    "3. Policy $\\pi=argmax_a Q(s,a)$.\n",
    "\n",
    "Loop for each episode:\n",
    "\n",
    "$$\\begin{align}\n",
    "&\\text{choose a start state } S \\\\ \n",
    "&\\text{Loop for each step in the episode}: \\\\\n",
    "& \\ \\ \\ \\ \\  \\text{  Take action } A \\text{as per state } s, \\pi \\\\\n",
    "& \\ \\ \\ \\ \\  \\text{  observe  }  R \\text{and next state  } s' \\\\\n",
    "& \\ \\ \\ \\ \\ \\  v(s) \\leftarrow v(s) + \\alpha [R + \\gamma v(s') -v(s)] \\\\\n",
    "& \\ \\ \\  \\ \\ \\ s \\leftarrow s'\n",
    "\\end{align}$$\n",
    "\n",
    "The difference that one sees in in the above algorithm is called the `TD error`. \n",
    "$$\\delta_t = R_{t+1} + \\gamma v(s_{t+1}) -v(s_t)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00200f8b",
   "metadata": {},
   "source": [
    "### SARSA\n",
    "\n",
    "SARSA on-Policy control\n",
    "\n",
    "1. Estimated state val: $q(s,a)=0,  \\forall s \\in \\mathbf{S}$,  $\\forall a \\in A $\n",
    "2. Policy: $\\pi=\\epsilon$-greedy policy\n",
    "3. Learning rate, stepsize $\\alpha \\in [0,1]$\n",
    "4. Discount factor $\\gamma \\in [0,1]$\n",
    "\n",
    "Loop for each episode:\n",
    "$$\\begin{align}\n",
    "&\\text{Start with a random S, choose A based on the policy.  }\\\\\n",
    "&\\text{Loop for each step unitl episode end:}  \\\\\n",
    "&\\ \\ \\ \\text{take action A and observe R and next state S'}\\\\\n",
    "&\\ \\ \\  \\text{choose A' using the }  \\epsilon -\\text{greedy policy using current Q} \\\\\n",
    "& \\ \\ \\  \\text{If S' is not terminal } \\\\\n",
    "& \\ \\ \\ \\ \\ \\ q(S,A) \\leftarrow q(S,A) + \\gamma [R + \\gamma q(S',A')-Q(S,A)] \\\\ \n",
    "&\\ \\ \\  \\text{Else: } \\\\ \n",
    "& \\ \\ \\ \\ \\ \\ q(S,A) \\leftarrow q(S,A) + \\gamma [R -Q(S,A)] \\\\\n",
    "&\\ \\ \\ S \\leftarrow S' ; A \\leftarrow A'\n",
    "\\end{align}$$\n",
    "\n",
    "Return the policy $\\pi$ based on the Q values\n",
    "\n",
    "\n",
    "### Off-Policy TD (Q- learning)\n",
    "\n",
    "In the off-policy algorithm, one again sample the action A' based on the state S'. In the off-policy TD, we will choice \n",
    "$$A'=argmax_{\\tilde{A}} q(S', \\tilde{A})$$\n",
    "\n",
    "1. Estimated state val: $q(s,a)=0,  \\forall s \\in \\mathbf{S}$,  $\\forall a \\in A $\n",
    "2. Policy: $\\pi=\\epsilon$-greedy policy\n",
    "3. Learning rate, stepsize $\\alpha \\in [0,1]$\n",
    "4. Discount factor $\\gamma \\in [0,1]$\n",
    "\n",
    "Loop for each episode:\n",
    "$$\\begin{align}\n",
    "&\\text{Start with a random S, choose A based on the } \\epsilon \\text{--greedy policy }.  \\\\\n",
    "&\\text{Loop for each step unitl episode end:}  \\\\\n",
    "&\\ \\ \\ \\text{take action A and observe R and next state S'}\\\\\n",
    "& \\ \\ \\  \\text{If S' is not terminal } \\\\\n",
    "& \\ \\ \\ \\ \\ \\ q(S,A) \\leftarrow q(S,A) + \\gamma [R + \\gamma \\max_{A'} q(S',A')-q(S,A)] \\\\ \n",
    "&\\ \\ \\  \\text{Else: } \\\\ \n",
    "& \\ \\ \\ \\ \\ \\ q(S,A) \\leftarrow q(S,A) + \\gamma [R -q(S,A)] \\\\\n",
    "&\\ \\ \\ S \\leftarrow S' ; \n",
    "\\end{align}$$\n",
    "\n",
    "Return the policy $\\pi$ based on the q values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26778f31",
   "metadata": {},
   "source": [
    "We are using the Max of estimates instead of the estimate of max along the way. \n",
    "\n",
    "Notice that the we are approximating the reward which is supposed to be in the expectation. However, by using the max of the estimate, we may get stuck in that value. \n",
    "\n",
    "One idea to overcome such difficulty is to use double-q learning. That is, we replace the $\\max_a q(s,a)$ with two value functions (neural network) $q_1(s, argmax_a q_2(s,a))$. \n",
    "\n",
    "We can also perform the expected SARSA by doing the following\n",
    "$$q(S,A) \\leftarrow q(S,A) + \\gamma [R + \\gamma \\sum_a \\pi(a|s')q(S',a)-q(S,A)]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95ba3ac",
   "metadata": {},
   "source": [
    "### Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c177cba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2977b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0218d456",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07852229",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e97418",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5913a6f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660c4b1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa488ac6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
