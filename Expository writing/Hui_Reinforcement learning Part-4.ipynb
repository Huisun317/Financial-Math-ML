{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16d0f83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc8abbe",
   "metadata": {},
   "source": [
    "## Policy Gradient Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d8b2fc",
   "metadata": {},
   "source": [
    "We calculate the gradient with respect to the control parameter when doing the optimization. \n",
    "\n",
    "$$\\pi(a|s)$$\n",
    "\n",
    "Policy and trajectory $\\tau$, and this is one path. \n",
    "\n",
    "$$s_1 \\rightarrow a_1 \\rightarrow  s_2 \\rightarrow a_2 \\rightarrow ... \\rightarrow s_{T-1} \\rightarrow a_{T-1} \\rightarrow s_T \\rightarrow a_T$$\n",
    "\n",
    "such chain depends on both the transition probability and the policy\n",
    "1. $p(s_{t+1}|s_t,a)$\n",
    "2. $\\pi_{\\theta}(a_t|s_t)$\n",
    "As a result, heurestically, the probability law of chains can be written in the following fashion\n",
    "$$\n",
    "\\begin{equation}\n",
    "p_{\\theta}(\\tau)= p(s_1) \\prod^T_{t=1} \\pi(a_{t}|s_{t}) p( s_{t+1} |a_{t}, s_{t} )\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "In this case, the objective function can be put in the following form: \n",
    "$$\n",
    "\\begin{equation}\n",
    "J(\\theta) = \\mathbb{E}_{\\tau \\sim p(\\theta)} [\\sum r(s_t, a_t)]\n",
    "\\end{equation}\n",
    "$$\n",
    "and the goal now is to find the $\\theta^*$ that maximize this reward. \n",
    "\n",
    "Now, we are going to take derivatives with respect to the parameter $\\theta$:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\nabla_{\\theta} J(\\theta) &= \\int \\nabla_{\\theta} p_{\\theta}(\\tau) r(\\tau) d\\tau \\\\\n",
    "&= \\int p_{\\theta}(\\tau) (\\nabla_{\\theta} \\log p_{\\theta}(\\tau) r(\\tau)) d \\tau \\\\ \n",
    "&= \\int  p_{\\theta}(\\tau) r(\\tau) \\nabla_{\\theta}\\lbrace log p(s_1)+ \\sum^T_{t=1}[\\log \\pi_{\\theta}(a_t|s_t) +\\log p(s_{t+1}|s_t,a_t)] \\rbrace \\\\\n",
    "&= \\mathbb{E}_{\\tau \\sim p_{\\theta}(\\tau)}[(\\sum^T_{t=1} \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t | s_t) ) (\\sum^T_{t=1} r(s_t,a_t))]\\\\\n",
    "& \\approx \\frac{1}{N} \\sum^N_{i=1} \\big \\lbrace \\sum^T_{t=1} \\nabla_{\\theta} \\log \\pi_{\\theta}(a^i_t | s^i_t) ) (\\sum^T_{t=1} r(s^i_t,a^i_t)) \\big \\rbrace\n",
    "\\end{align}\n",
    "$$\n",
    "And of course, the next step is to do a gradient descent \n",
    "$$\n",
    "\\begin{align}\n",
    "\\theta = \\theta + \\alpha \\nabla_{\\theta} J(\\theta)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Thus, the algorithm is natural: sample $(\\tau')$ a set of N trajectories from current policy $\\pi_{\\theta}(a_t|s)t)$ and update the parameters, and do the iterations. \n",
    "\n",
    "We comment here that the forumalation here follows a variational approach and does not depend on the Bellman's formulation\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c12b0bd",
   "metadata": {},
   "source": [
    "### Variance Reduction\n",
    "\n",
    "Noticing that the future states will not impact the historical ones (the world is casual), we make the following changes\n",
    "$$\n",
    "\\begin{align}\n",
    "\\nabla_{\\theta} J(\\theta) \n",
    "&= \\mathbb{E}_{\\tau \\sim p_{\\theta}(\\tau)}[\\sum^T_{t=1} (\\nabla_{\\theta} \\log \\pi_{\\theta}(a_t | s_t)  \\sum^T_{{t'}=t} r(s_{t'},a_{t'}))]\\\\\n",
    "& \\approx \\frac{1}{N} \\sum^N_{i=1}  \\sum^T_{t=1} \\big \\lbrace \\nabla_{\\theta} \\log \\pi_{\\theta}(a^i_t | s^i_t)  \\sum^T_{{t'}=t} r(s^i_{t'},a^i_{t'})) \\big \\rbrace\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Then, we can write the following \n",
    "$$Q^i_t:=\\sum^T_{{t'}=t} r(s^i_{t'},a^i_{t'})$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04049b0",
   "metadata": {},
   "source": [
    "## Regularization \n",
    "\n",
    "This is to ensure that the policy that we learn does not collapse to a single strategy.\n",
    "\n",
    "The regularity term is defined as \n",
    "$$H(x)= \\sum_x - p(x) \\log p(x)$$\n",
    "\n",
    "In this case, $p(x):= \\pi_{\\theta}(a^i_t|s^i_t)$ and higher entropy would mean the distribution is more spread out. \n",
    "\n",
    "$$\\begin{align}\n",
    "L(\\theta):=-\\frac{1}{N} \\sum^N_{i=1} \\bigg[  \\sum^T_{t=1} \\big \\lbrace \\nabla_{\\theta} \\log \\pi_{\\theta}(a^i_t | s^i_t)  \\sum^T_{{t'}=t} \\gamma^{t'-t}r(s^i_{t'},a^i_{t'})) \\big \\rbrace -\\beta \\sum_{a_i} \\pi_{\\theta}(a^i_t|s^i_t)\\log\\pi_{\\theta}(a^i_t|s^i_t) \\bigg]\n",
    "\\end{align}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a5c421",
   "metadata": {},
   "source": [
    "To perform variance reduction to make sure that the sampling is more effective, the following adjustment is proposed. \n",
    "$$\n",
    "\\begin{align}\n",
    "\\nabla_{\\theta} J(\\theta) = \\mathbb{E}_{\\tau \\sim p_{\\theta}(\\tau)}[\\big( \\sum^T_{t=1} \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t | s_t) \\big) (r(\\tau)-b(s_t))]\n",
    "\\end{align}\n",
    "$$\n",
    "Notice that $b(\\cdot)$ should only be a function of the state variable. \n",
    "\n",
    "We comment that reinforcement and all the variations are on-policy algorithms and so the trajectories created before the policy updates are dated. \n",
    "\n",
    "Consider the advantage function \n",
    "\n",
    "$$\\hat{Q}(s^i_t,a^i_t) -b^i(s_t)$$\n",
    "with before the $Q^i_t:=\\sum^T_{{t'}=t} r(s^i_{t'},a^i_{t'})$, now we can do a rollout, and take $$Q(s^i_t,a^i_t) =r(s^i_t,a^i_t)+V(s_{t+1})$$. Then, we have two approaches \n",
    "1. MC approach $$\\nabla_{\\theta} J(\\theta)=\\frac{1}{N} \\sum^N_{i=1}  \\sum^T_{t=1} \\bigg \\lbrace \\nabla_{\\theta} \\log \\pi_{\\theta}(a^i_t | s^i_t)  \\big[\\sum^T_{{t'}=t}  r(s^i_{t'},a^i_{t'})) - V(s_t) \\big] \\bigg \\rbrace$$\n",
    "2. TD approach $$\\nabla_{\\theta} J(\\theta)=\\frac{1}{N} \\sum^N_{i=1}  \\sum^T_{t=1} \\bigg \\lbrace \\nabla_{\\theta} \\log \\pi_{\\theta}(a^i_t | s^i_t)  \\big[ r(s^i_{t},a^i_{t})) + V(s_{t+1})- V(s_{t})\\big] \\bigg \\rbrace$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c739172e",
   "metadata": {},
   "source": [
    "## Algorithm\n",
    "Approximate $\\pi_{\\theta}(a|s)$ and $V_{\\phi}(s)$ using two different neural networks. \n",
    "\n",
    "Loop: \n",
    "1. Sample N trajectories from the current policy $\\pi_{\\theta}(a_t|s_t)$. \n",
    "2. Calculate the $\\hat{Q}^i_t=\\sum^T_{{t'}=t}  r(s^i_{t'},a^i_{t'})$. and fit it with the approximated neural network $V_{\\phi}(s)$ (L2 loss). \n",
    "\n",
    "$$\\phi= \\phi - \\beta \\nabla_{\\phi} L$$\n",
    "3. compute the cross-entropy loss: \n",
    "$$J(\\theta):=\\frac{1}{N} \\sum^N_{i=1}  \\sum^T_{t=1} \\bigg \\lbrace \\nabla_{\\theta} \\log \\pi_{\\theta}(a^i_t | s^i_t)  \\big[ r(s^i_{t},a^i_{t})) + V_{\\phi}(s_{t+1})- V_{\\phi}(s_{t})\\big] \\bigg \\rbrace$$\n",
    "Perform gradient descent on $\\theta$: \n",
    "$$\\theta=\\theta+ \\alpha \\nabla_{\\theta} J(\\theta) $$\n",
    "\n",
    "We comment that entropic regularization can also be applied to this current framework."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fe74fc",
   "metadata": {},
   "source": [
    "## A combination of Policy gradient descent and Q learning \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0099ed1e",
   "metadata": {},
   "source": [
    "1. Q-learning can be unstable sometimes. Though it is off policy and so the transitioning samples can be used multiple times. \n",
    "2. Learning policy directly gives much better convergence guarantees. However they are on-policy.\n",
    "\n",
    "Three methods are in scope. Deep deterministic policy gradients (DDPG), twin delayed DDPG (TD3) and soft actor critic (SAC). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9d01db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292434ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a860b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08aae922",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52bf603",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09597f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cfc850",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82271d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e101aba9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
