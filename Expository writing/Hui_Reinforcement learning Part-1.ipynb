{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9da0d095",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import sklearn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5959ece",
   "metadata": {},
   "source": [
    "# Markov Decision Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8626680d",
   "metadata": {},
   "source": [
    "Typically one has the following flow chart\n",
    "$$S_t \\rightarrow A_t \\rightarrow R_{t+1}, S_{t+1}$$\n",
    "where $S_t$ is the state process and $A_t$ is the action and $R_{t+1}$ is the reward. We point out that sometimes one may only have partial observation of the state process $S_t$, in this case the filtering method can be applied to attack such problem. However, for simplicity and to focus on the main idea here, we assume one can observe the entire state. \n",
    "\n",
    "What is interesting sometimes is that when the agent acts, the large rewards or actions can impact the environment. For example in the financial market, the major player can usually make a large impact on the market. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414919a6",
   "metadata": {},
   "source": [
    "A policy is a rule to make actions\n",
    "$$\\pi = \\mathbf{P}(a|s)$$\n",
    "A policy can be deterministic, in this case, it is simply a function of the state $s$. \n",
    "$$\\pi = f(s), \\  \\text{or} \\ \\ \\pi = \\sum^L_{i} a_i \\delta(s_i)$$\n",
    "\n",
    "`Should insert some pics here`\n",
    "\n",
    "The reward received and next state of the agent would then be a probability distribution over the possible values or reward  and next states\n",
    "$$\\mathbb{P}(s', r)=\\mathbb{P}(S_t=s', R_t=r| S_{t-1}=s, A_{t-1}=a)$$\n",
    "by tracing over the parameter $r$, we obtain the following probability distribution over the next state $s'$\n",
    "$$\\mathbb{P}(s'| S_{t-1}=s, A_{t-1}=a)=\\sum_{r}\\mathbb{P}(S_t=s', R_t=r| S_{t-1}=s, A_{t-1}=a)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d6dce1",
   "metadata": {},
   "source": [
    "## Bellman's equation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bb181b",
   "metadata": {},
   "source": [
    "We define $G_t$ to be the net lookahead reward:\n",
    "$$G_t=R_{t+1} +rR_{t+2}+r^2R_{t+3}+...+r^{T-t-1}R_{T}$$\n",
    "Then, clearly this function depends on the current state $S_t$ and the future trajectory (policy).\n",
    "Define the value function,\n",
    "$$\n",
    "\\begin{equation}\n",
    "v_{\\pi}(s)=\\mathbb{E}_{\\pi}[G_t|S_t=s] \\tag{1} \\label{valf}\n",
    "\\end{equation}\n",
    "$$\n",
    "One can also defined\n",
    "$$\n",
    "\\begin{equation}\n",
    "q_{\\pi}(s,a)=\\mathbb{E}_{\\pi}[G_t|S_t=s, A_t=a] \\tag{2} \\label{action_valf}\n",
    "\\end{equation}\n",
    "$$\n",
    "Then, we can write \n",
    "$$\n",
    "\\begin{align}\n",
    "G_t&=R_{t+1} +\\gamma(R_{t+2}+\\gamma R_{t+3}+...+\\gamma^{T-t-2}R_{T}) \\\\\n",
    "&=R_{t+1}+\\gamma G_{t+1}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Hence, \\eqref{valf} will take take the following form\n",
    "$$\n",
    "\\begin{align}\n",
    "v_{\\pi}(s) & = \\mathbb{E}_{\\pi}[G_t|S_t=s] \\\\ \n",
    "&= \\mathbb{E}_{\\pi}[R_{t+1}+rG_{t+1}|S_t=s]\\\\\n",
    "&= \\mathbb{E}_{\\pi}[R_{t+1}|S_t=s] +\\gamma \\mathbb{E}_{\\pi}[\\mathbb{E}_{\\pi}[G_{t+1}|S_{t+1}]|S_t=s] \\\\\n",
    "&=\\mathbb{E}_{\\pi}[R_{t+1}+\\gamma v_{\\pi}(S_{t+1})|S_t=s] \\\\\n",
    "&=\\sum_{a} \\pi(a|s) \\sum_{ r,s'} \\mathbb{P}(r,s'|a,s) (r + \\gamma  v_{\\pi}(s') )   \\tag{3} \\label{analytic_valf}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "For the same reason, one can write the `action value` function as follows: \n",
    "$$\n",
    "\\begin{align}\n",
    "q_{\\pi}&=\\mathbb{E}_{\\pi}[G_t|S_t=s,A_t=a] \\\\ \n",
    "&= \\sum_{r,s'} \\mathbb{P}(s',r|s,a)[r+\\gamma v_{\\pi}(s')] \\tag{4} \\label{analytic_action_valf}\n",
    "\\end{align}\n",
    "$$\n",
    "Then naturally, one has the following relationship:\n",
    "$$v_{\\pi}(s)=\\sum_{a}\\pi(a|s) q_{\\pi} (s,a)$$\n",
    "Then, the following relationship is attained\n",
    "$$\n",
    "\\begin{equation}\n",
    "q_{\\pi}(s,a)=\\sum_{ r,s'} \\mathbb{P}(r,s'|a,s) (r + \\gamma  \\sum_{a'}\\pi(a'|s') q_{\\pi} (s',a') ) \\tag{5} \\label{q_valf}\n",
    "\\end{equation}\n",
    " $$\n",
    " \n",
    " As a result, the following goal of maximization is in order\n",
    "$$\n",
    "\\begin{equation}\n",
    "V_*(s)=\\max_{\\pi}v_{\\pi}(s)\n",
    "\\end{equation}\n",
    "$$\n",
    "Also, by the defintion of $q_{\\pi}(s,a)$, we have that \n",
    "$$\n",
    "\\begin{equation}\n",
    "V_*(s)=\\max_{a}q_{\\pi^*}(s,a)\n",
    "\\end{equation}\n",
    "$$\n",
    "Hence, compared to \\eqref{analytic_valf} we have that the best policy is to put all the weight on the location $\\mathbb{P}(a|s)$ where the term on the right handside \\eqref{opt_1} is optimized\n",
    "$$\n",
    "\\begin{align}\n",
    "v_{*}(s) &= \\max_a \\sum_{ r,s'} \\mathbb{P}(r,s'|a,s) (r + \\gamma  v_{\\pi}(s') ) \\tag{6} \\label{opt_1} \\\\ \n",
    "q_*(s,a) &=  \\sum_{ r,s'} \\mathbb{P}(r,s'|a,s) (r + \\gamma  \\max_{a'}q_{*}(s',a') )  \\tag{7} \\label{opt_2}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Basically, the priciple is that the optimal policy \"must\" take\n",
    "$$\\pi^*(A=a'|s)=1$$\n",
    "if given $s$, the action $a'$ is the one that maximizes $v(s)$ among all other actions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe007e1",
   "metadata": {},
   "source": [
    "## Model Based Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4570f3",
   "metadata": {},
   "source": [
    "In this case, we assume that the `transition dynamics is known`. This is however typically a very strong assumption and may not be true under most scenarios.\n",
    "$$\\mathbb{P}(S_t=s', R_t=r| S_{t-1}=s, A_{t-1}=a)$$\n",
    "Problem like this are typically discrete, \n",
    "1. $n[A]$ is the total number of actions\n",
    "2. $n[S]$ is the total number of states\n",
    "3. The dictionary $P[s][a]$ assigns probability to such a pair.\n",
    "\n",
    "#### <ins>Policy Iterations</ins>\n",
    "Recall the Bellman's equation:\n",
    "$$\n",
    "\\begin{equation}\n",
    "v_{\\pi}=\\sum_{a} \\pi(a|s) \\sum_{ r,s'} \\mathbb{P}(r,s'|a,s) (r + \\gamma  v_{\\pi}(s') ) \\tag{BE} \\label{be}\n",
    "\\end{equation}\n",
    "$$\n",
    "And one should have total $n[S]$ of such equations. This forms a very large linear system when the cardinality is large. \n",
    "Since finding the solution of a large system is difficult in general, we seek to the contraction mapping idea. So the hope is that as $k \\rightarrow \\infty$, $v_k \\rightarrow v_{\\pi^*}$.\n",
    "This approach of finding all the value functions for the states is called `Policy Evalutions`. \n",
    "$$\n",
    "\\begin{equation}\n",
    "v_{k+1}(s) \\leftarrow \\sum_{a} \\pi(a|s) \\sum_{ r,s'} \\mathbb{P}(r,s'|a,s) (r + \\gamma  v_{k}(s') ) \\tag{BE update} \\label{be_u}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "#### <ins>Policy improvement and Iterations</ins>\n",
    "The state-action function can be computed as follows\n",
    "$$\n",
    "\\begin{equation}\n",
    "q(s,a) =  \\sum_{ r,s'} \\mathbb{P}(r,s'|a,s) (r + \\gamma  v_{\\pi}(s') )\n",
    "\\end{equation}\n",
    "$$\n",
    "Then we have \n",
    "$$v_{\\pi^*}(s)=\\max_{a} q_{\\pi^*}(s,a)$$\n",
    "which in turn leads to the following results:\n",
    "$$\\pi'(a|s)=argmax_{a} q(s,a)$$\n",
    "There is a `policy improvment theorem` that says: the values for all states under the new Policy $\\pi'$ is equal to or greateer than the state value function under the orignal policy unless it is the optimal one. \n",
    "\n",
    "Hence, one can update the policy in the following fashion: \n",
    "$$\\pi_0 \\rightarrow v_{\\pi_0} \\rightarrow \\pi_1 (\\text{from} \\ q(s,a)) \\rightarrow v_{\\pi_1} \\rightarrow ... \\rightarrow \\pi_{*} \\rightarrow v_{*} $$ \n",
    "And one should have consistently $v_{\\pi_{k+1}} \\geq v_{\\pi_k}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70eaf91a",
   "metadata": {},
   "source": [
    "### `Algorithm: Policy Iteration`:\n",
    "\n",
    "Set v(s)=0, s $\\in$ S\n",
    "\n",
    "Loop until $\\Delta \\leq \\delta$: \n",
    "$$\n",
    "\\begin{align}\n",
    "v'_{\\pi}(s) &\\leftarrow \\sum_{a} \\pi(a|s) \\sum_{ r,s'} \\mathbb{P}(r,s'|a,s) (r + \\gamma  v_{\\pi}(s') ),   \\ \\forall s \\in S \\\\\n",
    "\\Delta &= \\max(\\Delta, |v'(s)-v(s)|)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$v(s) \\leftarrow v(s')$ , $\\forall s \\in S$\n",
    "\n",
    "Policy changed $\\leftarrow$ False \n",
    "\n",
    "Loop $\\forall s \\in S$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{old-action} &\\leftarrow \\pi(s)\\\\ \n",
    "\\pi(s) &\\leftarrow argmax_{a} \\sum_{s', r} \\mathbb{P}(r,s'|a,s) (r + \\gamma  v_{\\pi}(s') )\\\\\n",
    "\\text{If} \\pi(s) &\\neq \\text{old-action}, \\ \\ \\text{Policy-changed = True }\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "One obvious downside of the algorithm above is that it requires two separate steps for an update of the policy. The value evalution step can take long by using the contraction idea.\n",
    "\n",
    "\n",
    "`The following approach is called \"value iteration\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b40154",
   "metadata": {},
   "source": [
    "### `Algorithm: Value Iteration`:\n",
    "\n",
    "Set v(s)=0, s $\\in$ S\n",
    "\n",
    "Loop until $\\Delta \\leq \\delta$: \n",
    "$$\n",
    "\\begin{align}\n",
    "v'_{\\pi}(s) &\\leftarrow \\max_a  \\sum_{ r,s'} \\mathbb{P}(r,s'|a,s) (r + \\gamma  v_{\\pi}(s') ),   \\ \\forall s \\in S \\\\\n",
    "\\Delta &= \\max(\\Delta, |v'(s)-v(s)|)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Find the optimal policy\n",
    "$$\\pi(s) \\leftarrow argmax_{a} \\sum_{s', r} \\mathbb{P}(r,s'|a,s) (r + \\gamma  v(s') )$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f549651",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acb69d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4bd72f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6434ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6a0820",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
